{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JozeRamos/BigData/blob/main/BDC2024-HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data Computing 2024 - Homework 2 - Deadline: December 22$^{nd}$, 11.59pm"
      ],
      "metadata": {
        "id": "iLEDVWoS_dCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions for the homework\n",
        "  > Follow the instructions in the notebook carefully\n"
      ],
      "metadata": {
        "id": "y0YCjaUt_2Af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## General instructions\n",
        "  * Do NOT remove ANYTHING that is already in the notebook.\n",
        "  * Before you start, show the line number for the code cells:\n",
        "    * Go to Tools -> Settings -> Editor -> show line numbers\n",
        "  * The group in the second Homework **MUST BE EXACTLY THE SAME** as in Homework 1\n",
        "  * The software must be commented.\n",
        "  * About this notebook:\n",
        "    * Create a personal copy to be able to modify it: File > Save a copy in Drive\n",
        "    * Rename the copied notebook `BDC2024-HW2.ipynb`"
      ],
      "metadata": {
        "id": "GUjIJyz8_8fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Instructions for **code**:\n",
        "  * DO NOT add code cells\n",
        "  * DO NOT remove any code cells\n",
        "  * Only edit the code cells containing the comment `#YOUR CODE STARTS HERE#`. Within those cells:\n",
        "    * DO NOT remove ANYTHING that we have written (otherwise specified).\n",
        "    * Add your code between line `#YOUR CODE STARTS HERE#` and line `#YOUR CODE ENDS HERE#`; **DO NOT REMOVE THESE COMMENTS**\n",
        "\n",
        "> Ensure that the notebook can be faithfully reproduced by anyone (hint: pseudo random number generation).\n",
        "\n",
        "> **If you need to set a random seed, set it to `1224`.**\n",
        "\n",
        "[comment]: <> (#REMOVE_CELL#)"
      ],
      "metadata": {
        "id": "Tjom-aYsAj00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Instructions for **text**:\n",
        "* DO NOT add text cells\n",
        "* DO NOT remove text cells\n",
        "* DO NOT modify the text cells we have created.\n",
        "* Only edit text cells starting with the comment ------------YOUR TEXT STARTS HERE------------. Within those cells:\n",
        "  * Do NOT remove anything we have entered.\n",
        "  * **Add your own text after the line ------------YOUR TEXT STARTS HERE------------; DO NOT REMOVE THIS TEXT**\n",
        "  * For each cell, a specific delivery will be given. If you are given a maximum number of sentences to write, both the dot \".\" and the semicolon \";\" will be considered sentence dividers.\n",
        "\n",
        "[comment]: <> (#REMOVE_CELL#)"
      ],
      "metadata": {
        "id": "_s_NvP_DBCZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## *Evaluation*\n",
        "\n",
        "The homework will be evaluated based on the following criteria:\n",
        "\n",
        "1. **Code Quality**: correctness, readability (length, comments, unnecessary repetitions)\n",
        "2. **Quality of Textual Responses**: correctness, clarity, etc.\n",
        "3. **Quality of Produced Visualizations** (tables/figures/etc.): correctness, clarity, etc.\n",
        "4. **Quality**, **quantity** and **diversity** of experiments conducted, **results** achieved, etc. \\[if applicable; will be clearly indicated\\]\n",
        "5. **Timely Submission**: late submissions will result in significant point deductions\n",
        "6. **Plagiarism**: copying code/text from colleagues or online sources will result in significant point deductions \\[it will be clearly indicated if borrowing code from other sources is allowed\\]\n",
        "\n",
        "*You are expected to work in a principled way, being aware of what you are doing and why*\n",
        "\n",
        "[comment]: <> (#REMOVE_CELL#)"
      ],
      "metadata": {
        "id": "fHGXI-zXBLKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Composition\n",
        "Write down the list of group members. The format should be Last Name, First Name, Student ID. Group members should be sorted alphabetically by surname and placed on different lines.\n",
        "Example:\n",
        "\n",
        "Becchetti, Luca, 123456\n",
        "\n",
        "Siciliano, Federico, 987654\n",
        "\n",
        "**Remember:** your group in the second homework must be the same as in the first one."
      ],
      "metadata": {
        "id": "JQzZjn-JBV_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Submission (read carefully)\n",
        "\n",
        "1. This notebook is one of two parts of a Google Classroom assignment, hence it should be delivered using Google Classroom. Your delivery for Homework 2 will thus consist of two files (like for Homework 1): i) this notebook; ii) a pdf with your answers to the theoretical assignments.\n",
        "2. Each notebook should be delivered *only once*, by the student whose last name comes first in alphabetical order in your group. So for example, if the group consists of Luca Becchetti and Federico Siciliano, Luca Becchetti (and *he only*) will deliver the homework (Federico Siciliano might well pay attention that Luca Becchetti actually does so within the mandatory deadline :-))\n",
        "3. You can upload multiple files as part of the same assignment in Google Classroom. Please be sure that you only press the Classroom's \"hand in\" button (\"Consegna\" in Italian) after you have uploaded the final versions of this notebook and of the pdf containing your answers to the theoretical questions."
      ],
      "metadata": {
        "id": "mkXmH72vBowy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework 2 (notebook)\n",
        "We want to perform topic distillation/[keyword extraction](https://en.wikipedia.org/wiki/Keyword_extraction) using the techniques seen in class, in particular: 1) $k$-means preceeded by dimensionality reduction techniques and 2) SVD or PCA alone, as seen in class. You will use the [20 newsgroups dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html), in particular the ```['comp.graphics', 'rec.motorcycles', 'rec.sport.baseball', 'sci.space', 'talk.religion.misc']```categories."
      ],
      "metadata": {
        "id": "--lB3UeWCG75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Software and dataset\n",
        "Use the cell below to import **ALL** Python packages that you need for this homework.\n",
        "\n",
        "Add them in this cell as you proceed with the implementation\n"
      ],
      "metadata": {
        "id": "j7ny7DzOzFAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#REMOVE_OUTPUT#\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "#YOUR CODE STARTS HERE#\n",
        "import nltk\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from time import time\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.random_projection import SparseRandomProjection as srp\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 15#"
      ],
      "metadata": {
        "id": "fZmrdQkwzJiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a4aa4c-e70d-4236-8589-f19dcdc5a6f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, download the dataset as specified earlier"
      ],
      "metadata": {
        "id": "SA8vLWVq2ELF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "#List with all the categories\n",
        "categories = ['comp.graphics', 'rec.motorcycles', 'rec.sport.baseball', 'sci.space', 'talk.religion.misc']\n",
        "\n",
        "#Dataset\n",
        "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
        "                             shuffle=False, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 10#"
      ],
      "metadata": {
        "id": "n49l2ZXI2Ogm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - keyword extraction/topic distillation using $k$-means\n",
        "As a baseline, cluster documents using standard $k$-means. The number of clusters will be equal to the number of categories you downloaded. Quality will be measured as follows:\n",
        "- Soundness of identified keywords: build word clouds of 20 most important keywords for each cluster\n",
        "- Time efficiency\n",
        "- Clustering quality with respect to true labels: [Adjusted Rand Index](https://en.wikipedia.org/wiki/Rand_index) (see scikit-learn documentation about)\n",
        "\n",
        "This part must run within a maximum of **15** minutes. To meet this constraint, approximations can be made (using fewer documents, reducing dictionary size). However, these approximations **must be explicitly indicated** in the code."
      ],
      "metadata": {
        "id": "0BPC3hfC0EGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1\n",
        "If you need to, prepare here helper functions to run your computations."
      ],
      "metadata": {
        "id": "xLR_WVyh4cb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "def lemmatization(dataset):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for i in range(len(dataset.data)):\n",
        "      word_list = word_tokenize(dataset.data[i])\n",
        "      lemmatized_doc = \"\"\n",
        "      for word in word_list:\n",
        "          lemmatized_doc = lemmatized_doc + \" \" + lemmatizer.lemmatize(word)\n",
        "      dataset.data[i] = lemmatized_doc\n",
        "  return dataset\n",
        "\n",
        "def K_Means(X):\n",
        "  km = KMeans(n_clusters=true_k, init='k-means++', n_init=20, max_iter=100) #change to 300\n",
        "  t0 = time()\n",
        "  km.fit(X)\n",
        "  K_Time = time() - t0\n",
        "  return [km, K_Time]\n",
        "\n",
        "def print_centroids(km,true_k):\n",
        "  centroids = km.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
        "  terms = vectorizer.get_feature_names_out()\n",
        "  for i in range(true_k):\n",
        "      print(\"Cluster %d:\" % i, end='')\n",
        "      for ind in centroids[i, :20]:\n",
        "          print(' %s' % terms[ind], end='')\n",
        "      print()\n",
        "  return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 40#"
      ],
      "metadata": {
        "id": "b2LxMygm4iOT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2\n",
        "Text pre-processing and vectorization goes here"
      ],
      "metadata": {
        "id": "R60nnZvM4__x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "labels = dataset.target\n",
        "true_k = len(np.unique(labels)) ## This should be 5 in this example\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "dataset = lemmatization(dataset)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english') ## Corpus is in English\n",
        "X = vectorizer.fit_transform(dataset.data).toarray() ## X is a sparse matrix --> we make it dense for fair comparison\n",
        "\n",
        "transformer = srp(n_components=1000, dense_output=True) # Using a dense representation for the matrix\n",
        "X_proj = transformer.fit_transform(X)\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 20#"
      ],
      "metadata": {
        "id": "e_0zT2iK5I6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd34232-e38f-4b7f-97bd-97d7e8515eb3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.3\n",
        "Initiate time calculation for Part 1."
      ],
      "metadata": {
        "id": "ak-WJ9b06KGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "start_time = time()\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 10#"
      ],
      "metadata": {
        "id": "w6qq7CVF8CvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1.4\n",
        "Run $k$-means"
      ],
      "metadata": {
        "id": "1Udh3UpS6r7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "[km,K_Time] = K_Means(X)\n",
        "\n",
        "\n",
        "[km_proj,K_Time_proj] = K_Means(X_proj)\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 10#"
      ],
      "metadata": {
        "id": "V02_Ov6R6ypH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print Adjusted Rand Index and Time efficiency."
      ],
      "metadata": {
        "id": "hS3KgJBO7A9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYnNUBbBHSn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e1fbab-351b-4fde-c29d-b14ad60d8bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand-Index for standart KMeans: 0.168\n",
            "Time efficiency for standart KMeans: 238.250s\n",
            "Adjusted Rand-Index for KMeans with random projections first: 0.122\n",
            "Time efficiency for KMeans with random projections first: 7.039s\n"
          ]
        }
      ],
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "#print the results for standart KMeans\n",
        "print(\"Adjusted Rand-Index for standart KMeans: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
        "print((\"Time efficiency for standart KMeans: %0.3fs\" % K_Time))\n",
        "\n",
        "#print the results for KMeans with random projections first\n",
        "print(\"Adjusted Rand-Index for KMeans with random projections first: %.3f\" % metrics.adjusted_rand_score(labels, km_proj.labels_))\n",
        "print((\"Time efficiency for KMeans with random projections first: %0.3fs\" % K_Time_proj))\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 10#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print, for each cluster, wordcloud of 20 most important keyworkds."
      ],
      "metadata": {
        "id": "3YffWFio7VGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "print(\"Wordcloud of 20 most important keywords for the standart KMeans:\\n\")\n",
        "print_centroids(km.cluster_centers_,true_k)\n",
        "\n",
        "centroids = np.zeros((true_k, X.shape[1])) # Initializing true_k centroid arrays\n",
        "cluster_sizes = [0]*true_k # For each cluster, the number of points it contains, needed for taking average\n",
        "for i in range(X_proj.shape[0]):\n",
        "    index = int(km.labels_[i]) # index is the index of the cluster the i-th point belongs to\n",
        "    centroids[index] += X[i] # Adding component-wise\n",
        "    cluster_sizes[index] += 1\n",
        "for i in range(true_k):\n",
        "    centroids[i] = centroids[i]/cluster_sizes[i] # Computing centroids: take sum and divide by cluster size\n",
        "\n",
        "print(\"\\nWordcloud of 20 most important keywords for the KMeans with random projections first:\\n\")\n",
        "print_centroids(centroids,true_k)\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 20#"
      ],
      "metadata": {
        "id": "3vqoYAKf85J5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2f5808-1c65-4aef-b2e3-21f7c28d1177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wordcloud of 20 most important keywords for the standart KMeans:\n",
            "\n",
            "Cluster 0: wa space people god think just like say did time thing know ha right jesus doe nasa launch year way\n",
            "Cluster 1: bike wa ride dog dod riding just like motorcycle ve mile car turn time rider road new know good honda\n",
            "Cluster 2: file image thanks format program graphic know gif ftp color software bit looking advance help graphics hi 3d use need\n",
            "Cluster 3: game year team wa player hit baseball run pitcher win ha think braves season pitch good pitching time fan ball\n",
            "Cluster 4: wa just like ha know list think did edu ve good doe got make time com new sure thanks ll\n",
            "\n",
            "Wordcloud of 20 most important keywords for the KMeans with random projections first:\n",
            "\n",
            "Cluster 0: wa space people god think just like say did time thing know ha right jesus doe nasa launch year way\n",
            "Cluster 1: bike wa ride dog dod riding just like motorcycle ve mile car turn time rider road new know good honda\n",
            "Cluster 2: file image thanks format program graphic know gif ftp color software bit looking advance help graphics hi 3d use need\n",
            "Cluster 3: game year team wa player hit baseball run pitcher win ha think braves season pitch good pitching time fan ball\n",
            "Cluster 4: wa just like ha know list think did edu ve good doe got make time com new sure thanks ll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize your results in 4 sentences at most."
      ],
      "metadata": {
        "id": "GReXoSdb9IU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------YOUR TEXT STARTS HERE------------\n"
      ],
      "metadata": {
        "id": "hP0MKciE9Rhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - keyword extraction/topic distillation using Latent Semantic Analysis\n",
        "We use Truncated SVD as an alternative baseline. In this case, we use the first $k$ components (singular vectors, with $k = 5$ in our case) of the Truncated SVD as descriptions of the latent topics and we identify the terms that are the most important for each component, as seen in class. Quality will be measured as follows:\n",
        "- Soundness of identified keywords: build word clouds of 20 most important keywords for each cluster\n",
        "- Time efficiency\n",
        "\n",
        "This part must run within a maximum of **15** minutes. To meet this constraint, approximations can be made (using fewer documents, reducing dictionary size). However, these approximations **must be explicitly indicated** in the code."
      ],
      "metadata": {
        "id": "_JfRjx4F9X6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1\n",
        "If you need to, prepare here helper functions to run your computations."
      ],
      "metadata": {
        "id": "jYS4O_wcAAy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 40#"
      ],
      "metadata": {
        "id": "eCSRrZGSAAy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2\n",
        "If you need to perform further text pre-processing and/or vectorization or redo it from scratch do it here. In this case, comment your code below to explain why you are doing this. Otherwise, leave the next cell empty."
      ],
      "metadata": {
        "id": "el8QVtBVAAy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 20#"
      ],
      "metadata": {
        "id": "dKKQgFKBAAzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3\n",
        "Initiate time calculation for Part 2."
      ],
      "metadata": {
        "id": "rPDzmfq6AAzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "start_time2 = time()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 10#"
      ],
      "metadata": {
        "id": "QabjsuMjAAzC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.4\n",
        "Perform Latent Semantic Analysis"
      ],
      "metadata": {
        "id": "46SFxov4AAzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "svd = TruncatedSVD(true_k)\n",
        "\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "Y = lsa.fit_transform(X)\n",
        "LSA_time = time() - start_time2\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 10#"
      ],
      "metadata": {
        "id": "LO_3SotfAAzD",
        "outputId": "fb1cd3fb-e220-45cf-c1a9-577437bee00a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f1ca22472794>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mLSA_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#YOUR CODE ENDS HERE#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \"\"\"\n\u001b[1;32m    715\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    584\u001b[0m             )\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    587\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0;34mf\" n_features({X.shape[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 )\n\u001b[0;32m--> 244\u001b[0;31m             U, Sigma, VT = randomized_svd(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     Q = randomized_range_finder(\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_random\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# singular vectors of A in Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_lu.py\u001b[0m in \u001b[0;36mlu\u001b[0;34m(a, permute_l, overwrite_a, check_finite, p_indices)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m def lu(a, permute_l=False, overwrite_a=False, check_finite=True,\n\u001b[0m\u001b[1;32m    187\u001b[0m        p_indices=False):\n\u001b[1;32m    188\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print Time efficiency."
      ],
      "metadata": {
        "id": "Hbw5YGCdAAzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcNROMqnAAzE"
      },
      "outputs": [],
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "print((\"Time efficiency for KMeans with random projections first: %0.3fs\" % LSA_time))\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 7#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print, for each cluster, wordcloud of 20 most important keyworkds."
      ],
      "metadata": {
        "id": "q3DsY8vfAAzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 20#"
      ],
      "metadata": {
        "id": "AydEaLGbAAzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize your results in 4 sentences at most."
      ],
      "metadata": {
        "id": "wawG4E9wAAzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------YOUR TEXT STARTS HERE------------\n"
      ],
      "metadata": {
        "id": "-jfnGBo4BZL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - keyword extraction/topic distillation using Truncated SVD + $k$-means\n",
        "In this case, we i) first embed documents, i.e., project them onto the space spanned by the first $m$ singular vectors (for a suitable choice of $m$, see further) and then ii) we cluster the $m$-dimensional vectors thus obtained using $k$-means as before. Quality will be measured as follows:\n",
        "- Clustering quality with respect to true labels: Adjusted Rand Index (see scikit-learn documentation about)\n",
        "- Soundness of identified keywords: build word clouds of 20 most important keywords for each cluster\n",
        "- Time efficiency\n",
        "\n",
        "To determine the number $m$ of singular components to keep, use the elbow method to identify a first tentative value and then the adjusted Rand Index to find the right value. For example, if you plot total explained variance for $m$ ranging from $1$ to $100$ (or perhaps even 50), you are likely to see a clear elbow. Assume you identify 30 as a tentative value. To find the best choice for $m$, you can compute the adjusted Rand index of the corresponding clustering for all values of $m$ in a neighbourhood of $30$ (e.g., in the range $\\{25,\\ldots , 35\\}$) and pick the value for which the index is maximum. This is just a simple heuristic, given to you as an example. If you think you found a better one you are free to use it, but you should briefly explain how it works. You are encouraged to use more principled heuristics proposed in the literature to choose $m$, in which case you should provide references about.\n",
        "\n",
        "This part must run within a maximum of **20** minutes. To meet this constraint, approximations can be made (using fewer documents, reducing dictionary size). However, these approximations **must be explicitly indicated** in the code."
      ],
      "metadata": {
        "id": "f_OQZGS4CERh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1\n",
        "If you need to, prepare here helper functions to run your computations."
      ],
      "metadata": {
        "id": "TXNz2PZACERj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 40#"
      ],
      "metadata": {
        "id": "H8AtgciDCERj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2\n",
        "If you need to perform further text pre-processing and/or vectorization or redo it from scratch do it here. In this case, comment your code below to explain why you are doing this. Otherwise, leave the next cell empty."
      ],
      "metadata": {
        "id": "uVC9pQUgCERk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 20#"
      ],
      "metadata": {
        "id": "I69H9oxxCERl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3\n",
        "Initiate time calculation for Part 3."
      ],
      "metadata": {
        "id": "rs69_OZACERm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 10#"
      ],
      "metadata": {
        "id": "4Zy0owjvCERm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4\n",
        "Perform Truncated SVD and identify your best choice for the number $m$ of components to keep. Briefly explain the strategy you adopt to find $m$, using at most 5 lines in the text cell below. If you are simply following the heuristic I suggested above (which I did not test but might do a decent job), please just briefly state this."
      ],
      "metadata": {
        "id": "SiqjeHehCERm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------YOUR TEXT STARTS HERE------------\n"
      ],
      "metadata": {
        "id": "lyh17sSxFyh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, write your code to identify the best value of $m$."
      ],
      "metadata": {
        "id": "w2O92g-ZF7Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 33#"
      ],
      "metadata": {
        "id": "ZwWORCNsGAG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.5\n",
        "Perform Truncated SVD (again if you did not save the results of the previous computation) with the number $m$ of components identified above. Cluster the projections of the documents onto the first $m$ components using $k$-means ($k = 5$ in our case)."
      ],
      "metadata": {
        "id": "_tfnmsb3GQvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 20#"
      ],
      "metadata": {
        "id": "ca3oaXhLG0Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print Time efficiency."
      ],
      "metadata": {
        "id": "REIqg0A5CERn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Zywjqy8CERo"
      },
      "outputs": [],
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 7#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print, for each cluster, wordcloud of 20 most important keyworkds."
      ],
      "metadata": {
        "id": "IVmmreWgCERo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR CODE STARTS HERE#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#YOUR CODE ENDS HERE#\n",
        "#THIS IS LINE 20#"
      ],
      "metadata": {
        "id": "07ooG0GACERo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize your results in 4 sentences at most."
      ],
      "metadata": {
        "id": "0wL5W4e8CERo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------YOUR TEXT STARTS HERE------------\n"
      ],
      "metadata": {
        "id": "o4xa9QxgCERp"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}